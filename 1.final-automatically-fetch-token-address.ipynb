{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec4f0eb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 118\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Process each token in the list\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m full_token_id, token_name \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 118\u001b[0m     network, token_address \u001b[38;5;241m=\u001b[39m full_token_id\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    119\u001b[0m     process_token(network, token_address, token_name, other_tokens)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Display the updated token list\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "#final for addresses fetch of particuler token automatically\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from IPython.display import display\n",
    "\n",
    "# Define constants\n",
    "BASE_URL = 'https://api.geckoterminal.com/api/v2'\n",
    "HEADERS = {'Accept': 'application/json;version=20230302'}\n",
    "\n",
    "# Function to safely request data from the API\n",
    "def safe_request(url, headers, params=None):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(f'HTTP error: {err}')\n",
    "    except Exception as err:\n",
    "        print(f'Error: {err}')\n",
    "    return None\n",
    "\n",
    "# Function to get pools for a given token\n",
    "def get_pools_for_token(network, token_address):\n",
    "    endpoint = f\"/networks/{network}/tokens/{token_address}/pools\"\n",
    "    url = BASE_URL + endpoint\n",
    "    all_pools_data = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        params = {'page': page}\n",
    "        pools = safe_request(url, HEADERS, params=params)\n",
    "        if not pools or 'data' not in pools:\n",
    "            break\n",
    "        all_pools_data.extend(pools['data'])\n",
    "        page += 1\n",
    "    return all_pools_data\n",
    "\n",
    "# Function to extract relevant information and update JSON file\n",
    "def process_token(network, token_address, token_name, other_tokens):\n",
    "    all_pools = get_pools_for_token(network, token_address)\n",
    "\n",
    "    # Extract relevant information into a dictionary\n",
    "    data_dict = {\n",
    "        'base_token_id': [],\n",
    "        'quote_token_id': [],\n",
    "        'dex_id': [],\n",
    "        'name': [],\n",
    "        'id': [],\n",
    "        \n",
    "    }\n",
    "\n",
    "    for item in all_pools:\n",
    "        attributes = item['attributes']\n",
    "        volume_usd = attributes['volume_usd']\n",
    "        \n",
    "        data_dict['name'].append(str(attributes['name']))  # Convert to string\n",
    "        data_dict['base_token_id'].append(item['relationships']['base_token']['data']['id'])\n",
    "        data_dict['quote_token_id'].append(item['relationships']['quote_token']['data']['id'])\n",
    "        data_dict['dex_id'].append(item['relationships']['dex']['data']['id'])\n",
    "        data_dict['id'].append(item['id'])\n",
    "    \n",
    "\n",
    "    # Create a DataFrame from the dictionary\n",
    "    df = pd.DataFrame(data_dict)\n",
    "\n",
    "    # Filter the DataFrame to remove rows with three or more tokens\n",
    "    df_address_want = df[['name', 'base_token_id', 'quote_token_id']]\n",
    "    df_address_want = df_address_want[df_address_want['name'].astype(str).str.count('/') < 2]  # Convert to string before using .str accessor\n",
    "\n",
    "    # Detect WETH token address\n",
    "    weth_address = network + \"_\" + token_address\n",
    "\n",
    "    # Extract rows where WETH is either the base or quote token\n",
    "    weth_pairs = df_address_want[(df_address_want['base_token_id'] == weth_address) | (df_address_want['quote_token_id'] == weth_address)]\n",
    "\n",
    "    # Extract other token names and addresses\n",
    "    for _, row in weth_pairs.iterrows():\n",
    "        if row['base_token_id'] == weth_address:\n",
    "            other_token_address = row['quote_token_id']\n",
    "            other_token_name = row['name'].split(' / ')[1]\n",
    "        else:\n",
    "            other_token_address = row['base_token_id']\n",
    "            other_token_name = row['name'].split(' / ')[0]\n",
    "        \n",
    "        if other_token_address in other_tokens:\n",
    "            if other_tokens[other_token_address] != other_token_name:\n",
    "                print(f\"Conflict detected for address {other_token_address}: {other_tokens[other_token_address]} vs {other_token_name}\")\n",
    "                # Skip adding conflicting token names\n",
    "                continue\n",
    "        else:\n",
    "            other_tokens[other_token_address] = other_token_name\n",
    "            print(\"update\")\n",
    "\n",
    "    # Save to JSON file\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(other_tokens, f, indent=4)\n",
    "\n",
    "\n",
    "# Load existing token addresses and names from JSON file if it exists\n",
    "file_name = 'polygon_pos_addresses.json'#bsc_addresses.json\n",
    "if os.path.exists(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        other_tokens = json.load(f)\n",
    "        \n",
    "data=other_tokens\n",
    "\n",
    "# Load existing token addresses and names from JSON file if it exists\n",
    "file_name = 'polygon_pos_addresses.json'#bsc_addresses.json\n",
    "if os.path.exists(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        other_tokens = json.load(f)\n",
    "else:\n",
    "    other_tokens = {}\n",
    "\n",
    "# Process each token in the list\n",
    "for full_token_id, token_name in data.items():\n",
    "    network, token_address = full_token_id.split('_')\n",
    "    process_token(network, token_address, token_name, other_tokens)\n",
    "\n",
    "# Display the updated token list\n",
    "len(other_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ac34c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP error: 404 Client Error: Not Found for url: https://api.geckoterminal.com/api/v2/networks/polygon_pos/tokens/0x0d500b1d8e8ef31e21c99d1db9a6444d3adf1270/pools?page=11\n",
      "update\n",
      "update\n",
      "update\n",
      "Conflict detected for address polygon_pos_0x7ceb23fd6bc0add59e62ac25578270cff1b9f619: WETH 0.04% vs WETH 0.05%\n",
      "Conflict detected for address polygon_pos_0x7ceb23fd6bc0add59e62ac25578270cff1b9f619: WETH 0.04% vs WETH 0.05%\n",
      "update\n",
      "update\n",
      "Conflict detected for address polygon_pos_0x2791bca1f2de4661ed88a30c99a7a9449aa84174: USDC 0.05% vs USDC 0.041%\n",
      "Conflict detected for address polygon_pos_0xc2132d05d31c914a87c6611c10748aeb04b58e8f: USDT 0.05% vs USDT 0.041%\n",
      "update\n",
      "update\n",
      "update\n",
      "Conflict detected for address polygon_pos_0x7ceb23fd6bc0add59e62ac25578270cff1b9f619: WETH 0.04% vs WETH\n",
      "Conflict detected for address polygon_pos_0x7ceb23fd6bc0add59e62ac25578270cff1b9f619: WETH 0.04% vs WETH 0.3%\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "Conflict detected for address polygon_pos_0x7ceb23fd6bc0add59e62ac25578270cff1b9f619: WETH 0.04% vs WETH 0.3%\n",
      "update\n",
      "Conflict detected for address polygon_pos_0x2791bca1f2de4661ed88a30c99a7a9449aa84174: USDC 0.05% vs USDC\n",
      "update\n",
      "Conflict detected for address polygon_pos_0x7ceb23fd6bc0add59e62ac25578270cff1b9f619: WETH 0.04% vs WETH\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "Conflict detected for address polygon_pos_0xc2132d05d31c914a87c6611c10748aeb04b58e8f: USDT 0.05% vs USDT 0.3%\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "Conflict detected for address polygon_pos_0x2791bca1f2de4661ed88a30c99a7a9449aa84174: USDC 0.05% vs USDC 0.3%\n",
      "update\n",
      "Conflict detected for address polygon_pos_0xc2132d05d31c914a87c6611c10748aeb04b58e8f: USDT 0.05% vs USDT\n",
      "update\n",
      "Conflict detected for address polygon_pos_0xc2132d05d31c914a87c6611c10748aeb04b58e8f: USDT 0.05% vs USDT\n",
      "Conflict detected for address polygon_pos_0xc2132d05d31c914a87c6611c10748aeb04b58e8f: USDT 0.05% vs USDT\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "Conflict detected for address polygon_pos_0x8f3cf7ad23cd3cadbd9735aff958023239c6a063: DAI vs DAI 0.3%\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "Conflict detected for address polygon_pos_0x2791bca1f2de4661ed88a30c99a7a9449aa84174: USDC 0.05% vs USDC\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "Conflict detected for address polygon_pos_0x7ceb23fd6bc0add59e62ac25578270cff1b9f619: WETH 0.04% vs WETH\n",
      "update\n",
      "update\n",
      "update\n",
      "Conflict detected for address polygon_pos_0x1bfd67037b42cf73acf2047067bd4f2c47d9bfd6: WBTC 0.05% vs WBTC 0.3%\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "Conflict detected for address polygon_pos_0x2791bca1f2de4661ed88a30c99a7a9449aa84174: USDC 0.05% vs USDC\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "Conflict detected for address polygon_pos_0x7ceb23fd6bc0add59e62ac25578270cff1b9f619: WETH 0.04% vs WETH\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "Conflict detected for address polygon_pos_0x1bfd67037b42cf73acf2047067bd4f2c47d9bfd6: WBTC 0.05% vs WBTC\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "Conflict detected for address polygon_pos_0x1bfd67037b42cf73acf2047067bd4f2c47d9bfd6: WBTC 0.05% vs WBTC 0.024%\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "Conflict detected for address polygon_pos_0x7ceb23fd6bc0add59e62ac25578270cff1b9f619: WETH 0.04% vs WETH 0.05%\n",
      "update\n",
      "update\n",
      "update\n",
      "Conflict detected for address polygon_pos_0xc2132d05d31c914a87c6611c10748aeb04b58e8f: USDT 0.05% vs USDT\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "Conflict detected for address polygon_pos_0x8f3cf7ad23cd3cadbd9735aff958023239c6a063: DAI vs DAI 0.022%\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n",
      "update\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final for addresses fetch of particuler token automatically\n",
    "#for 2 spilt like polygon_pos_0xhfldsgrtdfhgjstdhfjhsgdfjgk\n",
    "\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from IPython.display import display\n",
    "\n",
    "# Define constants\n",
    "BASE_URL = 'https://api.geckoterminal.com/api/v2'\n",
    "HEADERS = {'Accept': 'application/json;version=20230302'}\n",
    "\n",
    "# Function to safely request data from the API\n",
    "def safe_request(url, headers, params=None):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(f'HTTP error: {err}')\n",
    "    except Exception as err:\n",
    "        print(f'Error: {err}')\n",
    "    return None\n",
    "\n",
    "# Function to get pools for a given token\n",
    "def get_pools_for_token(network, token_address):\n",
    "    endpoint = f\"/networks/{network}/tokens/{token_address}/pools\"\n",
    "    url = BASE_URL + endpoint\n",
    "    all_pools_data = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        params = {'page': page}\n",
    "        pools = safe_request(url, HEADERS, params=params)\n",
    "        if not pools or 'data' not in pools:\n",
    "            break\n",
    "        all_pools_data.extend(pools['data'])\n",
    "        page += 1\n",
    "    return all_pools_data\n",
    "\n",
    "# Function to extract relevant information and update JSON file\n",
    "def process_token(network, token_address, token_name, other_tokens):\n",
    "    all_pools = get_pools_for_token(network, token_address)\n",
    "\n",
    "    # Extract relevant information into a dictionary\n",
    "    data_dict = {\n",
    "        'base_token_id': [],\n",
    "        'quote_token_id': [],\n",
    "        'dex_id': [],\n",
    "        'name': [],\n",
    "        'id': [],\n",
    "    }\n",
    "\n",
    "    for item in all_pools:\n",
    "        attributes = item['attributes']\n",
    "        \n",
    "        data_dict['name'].append(str(attributes['name']))  # Convert to string\n",
    "        data_dict['base_token_id'].append(item['relationships']['base_token']['data']['id'])\n",
    "        data_dict['quote_token_id'].append(item['relationships']['quote_token']['data']['id'])\n",
    "        data_dict['dex_id'].append(item['relationships']['dex']['data']['id'])\n",
    "        data_dict['id'].append(item['id'])\n",
    "\n",
    "    # Create a DataFrame from the dictionary\n",
    "    df = pd.DataFrame(data_dict)\n",
    "\n",
    "    # Filter the DataFrame to remove rows with three or more tokens\n",
    "    df_address_want = df[['name', 'base_token_id', 'quote_token_id']]\n",
    "    df_address_want = df_address_want[df_address_want['name'].astype(str).str.count('/') < 2]  # Convert to string before using .str accessor\n",
    "\n",
    "    # Detect WETH token address\n",
    "    weth_address = network + \"_\" + token_address\n",
    "\n",
    "    # Extract rows where WETH is either the base or quote token\n",
    "    weth_pairs = df_address_want[(df_address_want['base_token_id'] == weth_address) | (df_address_want['quote_token_id'] == weth_address)]\n",
    "\n",
    "    # Extract other token names and addresses\n",
    "    for _, row in weth_pairs.iterrows():\n",
    "        if row['base_token_id'] == weth_address:\n",
    "            other_token_address = row['quote_token_id']\n",
    "            other_token_name = row['name'].split(' / ')[1]\n",
    "        else:\n",
    "            other_token_address = row['base_token_id']\n",
    "            other_token_name = row['name'].split(' / ')[0]\n",
    "        \n",
    "        if other_token_address in other_tokens:\n",
    "            if other_tokens[other_token_address] != other_token_name:\n",
    "                print(f\"Conflict detected for address {other_token_address}: {other_tokens[other_token_address]} vs {other_token_name}\")\n",
    "                # Skip adding conflicting token names\n",
    "                continue\n",
    "        else:\n",
    "            other_tokens[other_token_address] = other_token_name\n",
    "            print(\"update\")\n",
    "\n",
    "    # Save to JSON file\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(other_tokens, f, indent=4)\n",
    "\n",
    "\n",
    "# Load existing token addresses and names from JSON file if it exists\n",
    "file_name = 'polygon_pos_addresses.json' # bsc_addresses.json\n",
    "if os.path.exists(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        other_tokens = json.load(f)\n",
    "        \n",
    "data = other_tokens\n",
    "\n",
    "# Load existing token addresses and names from JSON file if it exists\n",
    "file_name = 'polygon_pos_addresses.json' # bsc_addresses.json\n",
    "if os.path.exists(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        other_tokens = json.load(f)\n",
    "else:\n",
    "    other_tokens = {}\n",
    "\n",
    "# Process each token in the list\n",
    "for full_token_id, token_name in data.items():\n",
    "    split_index = full_token_id.rfind('_')\n",
    "    network = full_token_id[:split_index]\n",
    "    token_address = full_token_id[split_index + 1:]\n",
    "    process_token(network, token_address, token_name, other_tokens)\n",
    "\n",
    "# Display the updated token list\n",
    "len(other_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95bad82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polygon_pos\n",
      "0x0d500b1d8e8ef31e21c99d1db9a6444d3adf1270\n"
     ]
    }
   ],
   "source": [
    "file_name = 'polygon_pos_addresses.json' # bsc_addresses.json\n",
    "if os.path.exists(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        other_tokens = json.load(f)\n",
    "        \n",
    "data = other_tokens\n",
    "\n",
    "for full_token_id, token_name in data.items():\n",
    "    split_index = full_token_id.rfind('_')\n",
    "    network = full_token_id[:split_index]\n",
    "    token_address = full_token_id[split_index + 1:]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98d95134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate addresses found.\n"
     ]
    }
   ],
   "source": [
    "#for check if any json file have multiple addresses in json file\n",
    "\n",
    "import json\n",
    "\n",
    "# Load the JSON data from the file\n",
    "file_name = 'bsc_addresses.json'\n",
    "with open(file_name, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Initialize a dictionary to count occurrences of each address\n",
    "address_counts = {}\n",
    "\n",
    "# Count each address\n",
    "for address in data.keys():\n",
    "    if address in address_counts:\n",
    "        address_counts[address] += 1\n",
    "    else:\n",
    "        address_counts[address] = 1\n",
    "\n",
    "# Find addresses that appear more than once\n",
    "duplicates = {address: count for address, count in address_counts.items() if count > 1}\n",
    "\n",
    "# Print duplicate addresses\n",
    "if duplicates:\n",
    "    print(\"Duplicate addresses found:\")\n",
    "    for address, count in duplicates.items():\n",
    "        print(f\"Address: {address}, Count: {count}\")\n",
    "else:\n",
    "    print(\"No duplicate addresses found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a3abf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
